{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    " \n",
    "consumer_key = 'MBOUtzD5HA4uZKvU1QghiZtFq'\n",
    "consumer_secret = 'SlXd3eX4fuVrdvXq6S9hwVSR3AXDw5ALgIbC7LD71Nyn92h5du'\n",
    "access_token = '1151465584720109568-lF4zRc3gE9jmc3KVMCQ9rPxoPL64W4'\n",
    "access_secret = '4U79UP7SNLHoHqMYqyHmI0bCX0O3s6UTddVqnIiwaGaCR'\n",
    " \n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    " \n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyListener(StreamListener):\n",
    " \n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            with open('hastagvina.json', 'a') as f:\n",
    "                f.write(data)\n",
    "                return True\n",
    "        except BaseException as e:\n",
    "            print(\"Error on_data: %s\" % str(e))\n",
    "            \n",
    "        return True\n",
    " \n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        return True\n",
    " \n",
    "twitter_stream = Stream(auth, MyListener())\n",
    "twitter_stream.filter(track=['#VINA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Start of Data Visualitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)]\\(]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    "\n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "\n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('tyzara.json', 'r') as f:\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        tokens = preprocess(tweet['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 2737), (':', 1936), ('‚Ä¶', 1886), ('RT', 1849), ('#KPKJemputPaksaMendag', 1475), (',', 963), ('KPK', 599), ('@Prof_Kupi', 558), ('di', 488), ('@Kolak_VI', 477), ('!', 380), ('-', 347), ('@KPK_RI', 314), ('ini', 271), ('yg', 267), ('@NadLiviea', 265), ('?', 264), ('dan', 230), ('@hacknet__71', 215), ('@CNBLUE_9', 210), ('Mendag', 208), ('@idtodaydotco', 197), ('@bang_baret', 196), ('2', 194), ('Tagar', 193), ('tagar', 177), ('10', 176), ('ada', 175), ('Indonesia', 166), ('Sekjen', 161), ('@Berli_75', 152), ('saja', 151), ('dari', 150), ('PBNU', 148), ('ke', 146), ('Dipanggil', 138), ('3', 137), ('Trending', 137), ('untuk', 130), ('@dolirajabi4', 129), ('kalo', 128), ('#KpkJemputPaksaMendag', 126), ('sudah', 122), ('@Teh_TJEMPLUNG', 122), ('Jadi', 121), ('kalian', 117), ('Paling', 117), ('Twitter', 116), ('in', 115), ('x', 115), ('Atas', 115), (')', 113), ('1', 112), ('orang', 112), ('pada', 109), ('ga', 109), ('@Cte_cute', 107), ('https://t.co/Zrh8gkNx8r', 107), ('⁄©€í', 106), ('yang', 105), ('diam', 105), ('kita', 102), ('ÿßÿ≥', 98), ('@KingPurw4', 94), ('Masuk', 92), ('%', 91), ('@howsweb', 90), ('tuh', 89), ('üòÇ', 89), ('@geloraco', 88), ('@NengLambe', 88), ('jangan', 87), ('@Abetday02_day', 85), ('PSI', 85), ('@WesandriYusir', 84), ('tau', 84), ('petani', 83), ('aja', 83), ('@itsmeayugrt', 82), ('üòÅ', 80), ('trotoar', 80), ('@riri_februari', 79), ('milik', 79), ('@riri_februari2', 79), ('@republikaonline', 78), ('Mobil', 78), ('parkir', 78), ('bakal', 78), ('@Hansip_Ciraos', 78), ('@BinSukamdo', 77), ('@MSApunya', 77), ('dilaporkan', 77), ('warganet', 77), ('kerap', 77), ('menyerobot', 77), ('https://t.co/xzfi9LrOwc', 77), ('ü§£', 77), ('gini', 77), ('mangkir', 76), ('terbesar', 76)]\n"
     ]
    }
   ],
   "source": [
    "import operator \n",
    "import json\n",
    "from collections import Counter\n",
    " \n",
    "fname = 'kpk.json'\n",
    "with open(fname, 'r') as f:\n",
    "    count_all = Counter()\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        terms_all = [term for term in preprocess(tweet['text'])]\n",
    "        count_all.update(terms_all)\n",
    "    print(count_all.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    " \n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_stop = [term for term in preprocess(tweet['text']) if term not in stop]\n",
    "\n",
    "# Count terms only once, equivalent to Document Frequency\n",
    "terms_single = set(terms_all)\n",
    "# Count hashtags only\n",
    "terms_hash = [term for term in preprocess(tweet['text']) \n",
    "              if term.startswith('#')]\n",
    "# Count terms only (no hashtags, no mentions)\n",
    "terms_only = [term for term in preprocess(tweet['text']) \n",
    "              if term not in stop and\n",
    "              not term.startswith(('#', '@'))] \n",
    "              # mind the ((double brackets))\n",
    "              # startswith() takes a tuple (not a list) if \n",
    "              # we pass a list of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('RT', 424), ('#TerimaKasihZara', 423), ('‚Ä¶', 388), ('zara', 289), ('grad', 268), ('yang', 224), ('di', 222), ('sekarang', 181), ('2', 143), ('buat', 141), ('kamu', 133), ('16', 133), ('Ini', 128), ('sih', 127), ('banyak', 125), ('anak', 124), ('banget', 119), ('habis', 119), ('ngena', 118), ('minta', 118), ('tubirin', 118), ('@xxdkdb', 117), ('Kemarin', 117), ('Kasihan', 117), ('umur', 117), ('Zara', 115), ('JKT', 111), ('@zaraJKT48', 109), ('48', 109), ('dari', 96), ('jadi', 82), ('ke', 69), ('ini', 67), ('karna', 63), ('kalian', 63), ('@Rfky_prsty', 57), ('pisah', 57), ('korea', 57), ('Bima', 57), ('guru', 57), ('ngaji', 57), ('dikampunya', 57), ('https://t.co/C7HRtdXia5', 57), ('dan', 57), ('yg', 47), ('ya', 44), ('kau', 41), ('selalu', 39), ('aku', 38), ('tau', 35), ('jkt', 34), ('lagi', 34), ('akan', 34), ('48,', 33), ('‚ò∫', 33), ('#TerimakasihZara', 32), ('atau', 31), ('baru', 30), ('sudah', 30), ('üò≠', 30), ('masalah', 29), ('pagi', 28), ('apa', 28), ('ada', 27), ('ku', 27), ('dengan', 27), ('Adhisty', 27), ('orang', 27), ('mau', 26), ('dia', 26), ('Selamat', 26), ('#cafepolitic', 26), ('tapi', 25), ('lulus', 24), ('Twitter', 23), ('mengumumkan', 23), ('kelulusannya', 22), ('udah', 22), ('semoga', 21), ('sukses', 21), ('lebih', 21), ('nusantara', 21), ('saja', 21), ('film', 20), ('itu', 20), ('masa', 20), ('juga', 20), ('cantik', 20), ('sama', 19), ('nama', 19), ('menjadi', 18), ('untuk', 18), ('jangan', 18), ('kami', 18), ('tidak', 17), ('usia', 17), ('Trending', 17), ('dikenal', 17), ('Jadi', 16), ('saya', 16)]\n"
     ]
    }
   ],
   "source": [
    "fname = 'tyzara.json'\n",
    "with open(fname, 'r') as f:\n",
    "    count_all = Counter()\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        terms_all = [term for term in preprocess(tweet['text'])]\n",
    "        terms_stop = [term for term in preprocess(tweet['text']) if term not in stop]\n",
    "        count_all.update(terms_stop)\n",
    "    print(count_all.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vincent\n",
    " \n",
    "word_freq = count_all.most_common(20)\n",
    "labels, freq = zip(*word_freq)\n",
    "data = {'data': freq, 'x': labels}\n",
    "bar = vincent.Bar(data, iter_idx='x')\n",
    "bar.to_json('term_freq.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar.to_json('term_freq.json', html_out=True, html_path='chart1.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m http.server 8000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualitation Berdasarkan Waktu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: FutureWarning: how in .resample() is deprecated\n",
      "the new syntax is .resample(...).sum()\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import json\n",
    "import vincent\n",
    "\n",
    "dates_ITAvWAL = []\n",
    "# f is the file pointer to the JSON data set\n",
    "fname = 'tyzara.json'\n",
    "with open(fname, 'r') as f:\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        # let's focus on hashtags only at the moment\n",
    "        terms_hash = [term for term in preprocess(tweet['text']) if term.startswith('#')]\n",
    "        # track when the hashtag is mentioned\n",
    "        if '#zara' in terms_hash:\n",
    "            dates_ITAvWAL.append(tweet['created_at'])\n",
    " \n",
    "# a list of \"1\" to count the hashtags\n",
    "ones = [1]*len(dates_ITAvWAL)\n",
    "# the index of the series\n",
    "idx = pandas.DatetimeIndex(dates_ITAvWAL)\n",
    "# the actual series (at series of 1s for the moment)\n",
    "ITAvWAL = pandas.Series(ones, index=idx)\n",
    " \n",
    "# Resampling / bucketing\n",
    "per_minute = ITAvWAL.resample('1Min', how='sum').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_chart = vincent.Line(per_minute)\n",
    "time_chart.axis_titles(x='Time', y='Freq')\n",
    "time_chart.to_json('time_chart.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_chart.to_json('time_chart.json', html_out=True, html_path='time_chart.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m http.server 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
